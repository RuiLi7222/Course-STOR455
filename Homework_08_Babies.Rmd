# Multiple-linear regression.

**Created by Robin Cunningham, UNC Chapel Hill**

**Modified by Jan Hannig, UNC Chapel Hill**

***HOMEWORK 8 - Predicting Baby-weight***
<br><br>
A Multi-linear regression analysis. **Please compose all answers in this R-Markdown document**.
<br><br>
1. A file containing data on 1236 live births can be found at
'https://drive.google.com/open?id=0B2lwGKhIFjYYbDY3eWVubEZzX28'. We will use this dataset to construct a multi-linear model for predicting birthweight from other variables.
<br><br>
a. Write and execute code to read the csv file 'babies.csv', assign it to the object 'babies' and summarize the variables.
```{r, eval=TRUE}
  babies <- read.csv("~/FILES/UNC Course/STOR Major/STOR 455/Work in Stor 455/Excel Work/babies.csv")
  summary(babies)
```
<br>
b. As you can see, there are 8 variables and a fair number of missing data points. Remove all cases with missing data and assign the resulting data frame to 'bbycomp'.
```{r, eval=TRUE}
  countNa <- which(rowSums(is.na(babies)) > 0)
  bbycomp <- babies[-countNa, ]
  summary(bbycomp)
```
<br>
c. A **Dataset Codebook** is a guide to what each of the variables represents. Note that for the purpose of this study, we will consider each variable to be numerical. Complete the comment box below to create a codebook for these data. Include units if you can figure them out.
```
The 'bbycomp' Dataset Codebook
Dataset contains eight variables
i. Case - The serial number of Cases
Variable Type: numeric
Mean:  624.8
Minimum: 1.0
Maximum: 1236.0
Standard Deviation: 356.7771
Based upon 1174 valid cases out of 1174 total cases.
```
```
ii. bwt - baby's weight in ounces at birth
Variable Type: numeric
Mean: 119.5
Minimun: 55.0
Maximum: 176.0
Standard Deviation: 18.32867
Based upon 1174 valid cases out of 1174 total cases.
```
```
iii. gestation - duration of pregnancy in days
Variable Type: numeric
Mean: 279.1
Minimum: 148.0
Maximum: 353.0
Standard Deviation: 16.01031
Based upon 1174 valid cases out of 1174 total cases.
```
```
iv. parity - parity indicator (first born = 1, later born = 0)
Variable Type: numeric
Mean: 0.2624
Minimum: 0.0000
Maximum: 1.0000
Standard Deviation: 0.4400999
Based upon 1174 valid cases out of 1174 total cases.
```
```
v. age - mother's age in years
Variable Type: numeric
Mean: 27.23
Minimum: 15.00
Maximum: 45.00
Standard Deviation: 5.817839
Based upon 1174 valid cases out of 1174 total cases.
```
```
vi. height - mother's height in inches
Variable Type: numeric
Mean: 64.05 
Minimum: 53.00
Maximum: 72.00
Standard Deviation: 2.526102
Based upon 1174 valid cases out of 1174 total cases.
```
```
vii. weight - mother's weight in pounds(during pregnancy)
Variable Type: numeric
Mean: 128.5
Minimum: 87.0
Maximum: 250.0
Standard Deviation: 20.73428
Based upon 1174 valid cases out of 1174 total cases.
```
```
viii. smoke - indicator for whether mother smokes (1 = yes, 0 = no)
Variable Type: numeric
Mean: 0.391
Minimum: 0.000
Maximum: 1.000
Standard Deviation: 0.4881759
Based upon 1174 valid cases out of 1174 total cases.
```
<br>
d.Do some exploratory analysis by looking at histograms of the 7 variables and plots of bwt versus each of the six explanatory variables. In the comment box below, make a note of any concerns. ***It will save some typing to assign the right variables to `Y, X1, ..., X6`, so I did that for you.***
```{r, eval = TRUE}

#Assign Short variable names
Y<- bbycomp$bwt
X1 <- bbycomp$gestation
X2 <- bbycomp$parity
X3 <- bbycomp$age
X4 <- bbycomp$height
X5 <- bbycomp$weight
X6 <- bbycomp$smoke

#Histograms
hist(Y) 
hist(X1)
hist(X2)
hist(X3)
hist(X4)
hist(X5)
hist(X6)

#Plots of Y versus X_i
plot(X1, Y)
plot(X2, Y)
plot(X3, Y)
plot(X4, Y)
plot(X5, Y)
plot(X6, Y)
```
```
Comments:
Based on the output of histograms, there are several concerns:
In hist(X1), duration of pregnancy in days below 200 or above 350 are quite abnormal. 
In hist(X3), mother's age of 15 is quite abnormal.
In hist(X5), mother's weight in pounds(during pregnancy) above 200 seem to be abnormal.

According to the output of plots, there are several concerns:
There is no clear relationship between Y ~ X2, Y ~ X3, Y ~ X4, Y ~ X6.
There are apparent outliers in plot(X1, Y) and plot(X4, Y).
```
<br>
e. Run the full model using all of the other variables (besides Case) to explain Birthweight (bwt). Store the model as `full.lm` and create a summary of the model.
```{r, eval=TRUE}
  full.lm <- lm(Y ~ X1 + X2 + X3 + X4 + X5 + X6, data = bbycomp)
  summary(full.lm)
```
<br>
f. Use the summary to conduct an ANOVA test to see if at least one of the coefficients is significantly different from zero. State the results in the comment box below.

$$Null \hspace{0.2cm} Model: Y = \beta_0  \hspace{12.75cm}$$
$$Full \hspace{0.2cm} Model: Y = \beta_0 + \beta_1*X1 + \beta_2*X2 + \beta_3*X3 + \beta_4*X4 + \beta_5*X5 + \beta_6*X6$$
$$H_0: \hspace{0.2cm} \beta_1 = \beta_2 = \beta_3 = \beta_4 = \beta_5 = \beta_6 = 0 \hspace{9.5cm}$$
$$H_A: \hspace{0.2cm}At \hspace{0.2cm} least \hspace{0.2cm} one \hspace{0.2cm} \beta_i \hspace{0.2cm} not \hspace{0.2cm} equal \hspace{0.2cm} to \hspace{0.2cm} 0 \hspace{0.2cm} (i=1, 2, 3, 4, 5, 6)\hspace{5.5cm}$$
```
According to the summary output, F-statistic = 67.61, p-value < 2.2e-16 < 0.001. Therefore, we have 0.001 level evidence to reject the null hypothesis in favor of the alternative hypothesis and support that at least one of the coefficient is significantly different from zero.
```
<br>
g. Now perform backward elimination in the following manner: First, eliminate the predictor whose removal causes the greatest improvement in adjusted R-squared. Continue in this manner until removing any remaining predictors causes Adjusted R-squared to fall. <br>
Begin by finding the 5-predictor model that increases adjusted R-squared by the most. Include the model and summary in the codebox below.
```{r, eval=TRUE}
  Ev_lm5 <- lm(Y ~ X1 + X2 + X4 + X5 + X6, data = bbycomp)
  summary(Ev_lm5)
```
<br>
h. Should we stick with the 6-predictor model or continue? Explain.
```
According to the output of 5-predictor model, the Adjusted R-squared is 0.2548 which is larger than the 6-predictor model's Adjusted R-squared 0.2541. Therefore, we should not stick with the 6-predictor model and continue.
```
<br>
i. Now find the best 4-variable model using the same criterion and include it in the code box below. Include a summary of the model.
```{r, eval=TRUE}
  Ev_lm4 <- lm(Y ~ X1 + X2 + X4 + X6, data = bbycomp)
  summary(Ev_lm4)
```
<br>
j. According to the Adj. R-squared criterion, should we stick with 5-predictors or continue? Explain.
```
Based on the output of 4-predictors model, the Adjusted R-squared is 0.2529 which is smaller than the 5-predictor model's Adjusted R-squared 0.2548. Therefore, we should stick with 5-predictors and do not continue.
```
***Note: even though our criterion says to stick with 5 predictors, I would seriously consider dropping X5 anyway, because the p-value is very close to 0.05 and we have lots of predictors. (Think about why having lots of predictors matters for this!) Also, the value of Adjusted R-squared is only reduced slightly and a parsimonious model is easier to understand and more robust for predictions***
<br>
k. Using the best 5-predictor model that you found, find a 95% confidence interval for the average birthweight among all babies for which (gestation, parity, age, height, weight, smoke) = (290, 1, 22, 60, 110, 0). (One line of code will do it.)
```{r, eval= TRUE}
  predict(Ev_lm5, data.frame(X1 = 290, X2 = 1, X4 = 60, X5 = 110, X6 = 0), interval = "confidence", level = 0.95)
```
<br>
l. Using the best 5-predictor model that you found, find a 95% confidence interval for the birthweight of the next baby for which (gestation, parity, age, height, weight, smoke) = (290, 1, 22, 60, 110, 0). (Again, don't make it hard ... one line.)
```{r, eval = TRUE}
  predict(Ev_lm5, data.frame(X1 = 290, X2 = 1, X4 = 60, X5 = 110, X6 = 0), interval = "predict", level = 0.95)
```
<br>
m. In plain English, interpret the coefficients in the least squares model for `height` and `smoke`.

```
Based on the output of 5-predictor model, when minimizing the sum of squares of residuals:

The estimate coefficients of height is 1.15497. When increases one inch of mother's height, the baby's weight in ounces at birth will increase 1.15497 ounces.

Also, there is 0.001 level evidence supporting that the height coefficient is not equal to zero.

The estimate coefficients of smoke is -8.39390. As variable 'smoke' only have two value (0 and 1), compared to 0, when the value is 1 the baby's weight in ounces at birth will decrease 8.39390 ounces.

Also, there is 0.001 level evidence supporting that the weight coefficient is not equal to zero.
```
<br><br>
2. Run diagnostics on the final 5-predictor model you selected. Include appropriate residual plots and your comments on the quality and usefulness of the fit. (Make your own codeboxes and comment boxes.)
<br><br>

```{r, eval= TRUE}
  res <- resid(Ev_lm5)
  plot(X1, res)
  plot(X2, res)
  plot(X4, res)
  plot(X5, res)
  plot(X6, res)
  plot(Ev_lm5)
```
```
In plot(X1, res), the points scatter randomly, showing that residuals do not depend on the gestation.

In plot(X2, res), the points scatter on two lines, X2 = 0 and X2 = 1, showing that resisduals depend on the parity.

In plot(X4, res), the points scatter randomly on lines between X = 55 and X = 75, showing that residuals do not depend on the height.

In plot(X5, res), the points scatter randomly, showing that residuals do not depend on the weight.

In plot(X6, res), the points scatter on two lines, X6 = 0 and X6 = 1, showing that resisduals depend on the smoke.

In Residuals vs Fitted and Scale - Location, the points scatter radomly. Also, the Q-Q plot nearly forms a line. Therefore, residuals are normally distributed and have the same variance for each Xi.
```
<br><br>
3. In the plots you created before doing any regressions, there were apparent outliers with regard to both X1 and X4. Without doing the work, say what steps you would take to evaluate whether we should consider removing these outliers. (Your own comment box.)
<br><br>
```
In order to evaluate whether I should remove these outliers, I will do following steps:

First, do R command on plot(X1, Y) and plot(X4, Y).

Second, operate simple linear regression models as lm(Y ~ X1) and lm(Y ~ X4).

Third, add a line of linear regression to the plot respectively, coding as abline(lm(Y ~ X1)) and abline(lm(Y ~ x4)).

Fourth, compared the regression line and the plot, evaluating if any outliers have great influence on the line.

Fifth, if the influence is large, then I will remove these outliers; if the influence is little, then I will keep these outliers.

Finally, whichever decision I make, I will document the fact and my motivation.
```
<br><br>
4. Using `Forward Addition`, choose a "best" multilinear model for this data set. Begin by choosing the single predictor that gives the highest value of Adjusted R-squared and continue by adding variables until Adjusted R-squared has been maximized. ***Your answer should consist of a set of nested models with increasing Adjusted R-squared***
<br><br>
```{r, eval = TRUE}
  Fa_lm1 <- lm (Y ~ X1, data = bbycomp)
  summary(Fa_lm1)
  Fa_lm2 <- lm (Y ~ X1 + X6, data = bbycomp)
  summary(Fa_lm2)
  Fa_lm3 <- lm (Y ~ X1 + X6 + X4, data = bbycomp)
  summary(Fa_lm3)
  Fa_lm4 <- lm (Y ~ X1 + X6 + X4 + X2, data = bbycomp)
  summary(Fa_lm4)
  Fa_lm5 <- lm (Y ~ X1 + X6 + X4 + X2 + X5, data = bbycomp)
  summary(Fa_lm5)
```
<br><br>
5. For the sequence of nested models above, conduct an ANOVA test comparing each model to the previous, reduced model to see if the new coefficient is statistically different from zero compared to the reduced model. Show the code for each test and state the results.
<br><br>
```{r, eval = TRUE}
  Fa_lm0 <- lm(Y ~ 1, data = bbycomp)
  anova(Fa_lm0, Fa_lm1)
```
$$H_0: \hspace{0.2cm} \beta_1 = 0$$
$$H_A: \hspace{0.2cm} \beta_1 \neq 0$$
```
According to the output of anova(Fa_lm0, Fa_lm1), the p-value is smaller than 0.001. Therefore, we have 0.001 level evidence to reject the null hypothesis in favour of the alternative hypothesis. We can conclude that the coefficient of gestation(X1) is statistically different from zero compared to the reduced model.
```

```{r, eval = TRUE}
  anova(Fa_lm1, Fa_lm2)
```  
$$H_0: \hspace{0.2cm} \beta_6 = 0$$
$$H_A: \hspace{0.2cm} \beta_6 \neq 0$$
```
According to the output of anova(Fa_lm1, Fa_lm2), the p-value is smaller than 0.001. Therefore, we have 0.001 level evidence to reject the null hypothesis in favour of the alternative hypothesis. We can conclude that the coefficient of smoke(X6) is statistically different from zero compared to the reduced model.
```

```{r, eval = TRUE}  
  anova(Fa_lm2, Fa_lm3)
```  
$$H_0: \hspace{0.2cm} \beta_4 = 0$$
$$H_A: \hspace{0.2cm} \beta_4 \neq 0$$

```
According to the output of anova(Fa_lm2, Fa_lm3), the p-value is smaller than 0.001. Therefore, we have 0.001 level evidence to reject the null hypothesis in favour of the alternative hypothesis. We can conclude that the coefficient of height(X4) is statistically different from zero compared to the reduced model.
```
```{r, eval = TRUE}  
  anova(Fa_lm3, Fa_lm4)
```
$$H_0: \hspace{0.2cm} \beta_2 = 0$$

$$H_A: \hspace{0.2cm} \beta_2 \neq 0$$

```
According to the output of anova(Fa_lm3, Fa_lm4), the p-value is smaller than 0.001. Therefore, we have 0.001 level evidence to reject the null hypothesis in favour of the alternative hypothesis. We can conclude that the coefficient of parity(X2) is statistically different from zero compared to the reduced model.
```

```{r, eval = TRUE}
  anova(Fa_lm4, Fa_lm5)
```
$$H_0: \hspace{0.2cm} \beta_5 = 0$$

$$H_A: \hspace{0.2cm} \beta_5 \neq 0$$

```
According to the output of anova(Fa_lm4, Fa_lm5), the p-value is smaller than 0.001. Therefore, we have 0.001 level evidence to reject the null hypothesis in favour of the alternative hypothesis. We can conclude that the coefficient of weight(X5) is statistically different from zero compared to the reduced model.
```
<br><br>
6. Would considering some interactions make sense? Try to add some interactions to the best models and see what happens.
<br><br>
```{r, eval = TRUE}
  Inter_lm <- lm(Y ~ X1 + X6 + X4 + X2 + X5 + X1 : factor(X6) + factor(X2) : X4)
  summary(Inter_lm)
```
```
I add the interactions of gestation & factor(smoke) and factor(parity) & height.
Based on the output above, the interactions increase the Adjusted R-squared from 0.2548 to 0.2633.
However, the interactions decrease the significance (p-value) of X2 and X5.
There is 0.001 level evidence significantly showing that the coefficient of joint variable X1 and X6 does not equal to zero.
There is weak evidence showing that the coefficient of joint variable X2(1) and X4 does not equal to zero.
```